{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sampled_data = pd.read_csv('sex_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_confusion_matrix(y_true, y_pred, group):\n",
    "    group = group.astype(bool)\n",
    "\n",
    "    y_true_group = y_true[group]\n",
    "    y_pred_group = y_pred[group]\n",
    "\n",
    "    # Calculate true positives, false positives, true negatives, and false negatives\n",
    "    tp = np.sum((y_pred_group == 1) & (y_true_group == 1))\n",
    "    tn = np.sum((y_pred_group == 0) & (y_true_group == 0))\n",
    "    fp = np.sum((y_pred_group == 1) & (y_true_group == 0))\n",
    "    fn = np.sum((y_pred_group == 0) & (y_true_group == 1))\n",
    "\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def EqualOpportunityDifference(y, pred, group_a, group_b):\n",
    "    tp_a, _, _, fn_a = calculate_confusion_matrix(y, pred, group_a)\n",
    "    tp_b, _, _, fn_b = calculate_confusion_matrix(y, pred, group_b)\n",
    "    tpr_a = tp_a / (tp_a + fn_a) if (tp_a + fn_a) > 0 else 0\n",
    "    tpr_b = tp_b / (tp_b + fn_b) if (tp_b + fn_b) > 0 else 0\n",
    "    return tpr_b - tpr_a\n",
    "\n",
    "def FalsePositiveRateBalance(y, pred, group_a, group_b):\n",
    "    _, tn_a, fp_a, _ = calculate_confusion_matrix(y, pred, group_a)\n",
    "    _, tn_b, fp_b, _ = calculate_confusion_matrix(y, pred, group_b)\n",
    "    fpr_a = fp_a / (fp_a + tn_a) if (fp_a + tn_a) > 0 else 0\n",
    "    fpr_b = fp_b / (fp_b + tn_b) if (fp_b + tn_b) > 0 else 0\n",
    "    return fpr_b - fpr_a\n",
    "\n",
    "def EqualisedOdds(y, pred, group_a, group_b):\n",
    "    return (EqualOpportunityDifference(y, pred, group_a, group_b) +\n",
    "            FalsePositiveRateBalance(y, pred, group_a, group_b)) / 2\n",
    "\n",
    "def PredictiveParityDifference(y, pred, group_a, group_b):\n",
    "    tp_a, _, fp_a, _ = calculate_confusion_matrix(y, pred, group_a)\n",
    "    tp_b, _, fp_b, _ = calculate_confusion_matrix(y, pred, group_b)\n",
    "    precision_a = tp_a / (tp_a + fp_a) if (tp_a + fp_a) > 0 else 0\n",
    "    precision_b = tp_b / (tp_b + fp_b) if (tp_b + fp_b) > 0 else 0\n",
    "    return precision_b - precision_a\n",
    "\n",
    "def StatisticalParityDifference(y, pred, group_a, group_b):\n",
    "    positive_rate_a = pred[group_a].mean()\n",
    "    positive_rate_b = pred[group_b].mean()\n",
    "    return positive_rate_b - positive_rate_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_e = (sampled_data['derived_sex'] == 'Male').astype(int)\n",
    "group_f = (sampled_data['derived_sex'] == 'Female').astype(int)\n",
    "\n",
    "bias_metrics = {\n",
    "    \"Equal Opportunity Difference\": EqualOpportunityDifference,\n",
    "    \"False Positive Rate Balance\": FalsePositiveRateBalance,\n",
    "    \"Equalised Odds\": EqualisedOdds,\n",
    "    \"Predictive Parity Difference\": PredictiveParityDifference,\n",
    "    \"Statistical Parity Difference\": StatisticalParityDifference\n",
    "}\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, precision_score, roc_auc_score\n",
    "\n",
    "# setup the metrics to be computed\n",
    "from sklearn import metrics\n",
    "perf_metrics = {\"Accuracy\": metrics.accuracy_score, \n",
    "                \"Precision\": metrics.precision_score, \n",
    "                \"Recall\": metrics.recall_score,\n",
    "                \"AUC\": metrics.roc_auc_score, \n",
    "                \"F1-Score\": metrics.f1_score,\n",
    "                }\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = sampled_data['action_taken']\n",
    "\n",
    "X = sampled_data.drop(['Unnamed: 0','action_taken', 'derived_race','derived_sex'], axis=1)\n",
    "\n",
    "test_set = 0.2\n",
    "seed = 123\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_set, random_state=seed, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/visharlya/.virtualenvs/py3cv4/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/visharlya/.virtualenvs/py3cv4/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/visharlya/.virtualenvs/py3cv4/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/visharlya/.virtualenvs/py3cv4/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/visharlya/.virtualenvs/py3cv4/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "\n",
    "lr = LogisticRegression(random_state=10, solver=\"lbfgs\", penalty=\"none\", max_iter=1000)\n",
    "mv = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "sex_metrics_all = pd.DataFrame()\n",
    "k, i = True, 1\n",
    "i = 1\n",
    "for (train, test) in mv.split(X, y):\n",
    "    lr.fit(X.iloc[train], y.iloc[train].values.ravel())\n",
    "    ypred_prob = lr.predict_proba(X.iloc[test]).ravel()[1::2] # get probabilities\n",
    "    ypred_class = lr.predict(X.iloc[test])\n",
    "\n",
    "    # compute performance metrics\n",
    "    metrics = []\n",
    "    for pf in perf_metrics.keys():\n",
    "        if pf == \"AUC\":\n",
    "            metrics += [[pf, perf_metrics[pf](y.iloc[test].values.ravel(), ypred_prob)]]\n",
    "        else:\n",
    "            metrics += [[pf, perf_metrics[pf](y.iloc[test].values.ravel(), ypred_class)]]\n",
    "\n",
    "    # concatenate results\n",
    "    df_m = pd.DataFrame(metrics, columns=[\"Metric\", \"Value\"])\n",
    "    df_m[\"Fold\"] = i\n",
    "    i += 1\n",
    "    if k:\n",
    "        df_metrics = df_m.copy()\n",
    "        k=0\n",
    "    else:\n",
    "        df_metrics = pd.concat([df_metrics, df_m.copy()], axis=0, ignore_index=True)\n",
    "\n",
    "    # Reset these lists inside the loop for each fold\n",
    "    sex_metrics = []\n",
    "\n",
    "    for bias in bias_metrics.keys():                                   \n",
    "        sex_metrics.append([bias, bias_metrics[bias](y.iloc[test].values.ravel(), ypred_class,\n",
    "                                        group_e[test], group_f[test])])\n",
    "\n",
    "        # Convert lists to DataFrames before concatenation\n",
    "        sex_df = pd.DataFrame(sex_metrics, columns=[\"Metric\", \"Value\"]).assign(Fold=i)\n",
    "\n",
    "\n",
    "\n",
    "    sex_metrics_all = pd.concat([sex_metrics_all, sex_df], axis=0)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "\n",
    "sex_summary = sex_metrics_all.pivot_table(index='Metric', values='Value', aggfunc=['mean', 'std'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AUC</th>\n",
       "      <td>0.984604</td>\n",
       "      <td>0.000522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.951635</td>\n",
       "      <td>0.000738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-Score</th>\n",
       "      <td>0.971444</td>\n",
       "      <td>0.000475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.982941</td>\n",
       "      <td>0.001102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.960217</td>\n",
       "      <td>0.001894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean       std\n",
       "              Value     Value\n",
       "Metric                       \n",
       "AUC        0.984604  0.000522\n",
       "Accuracy   0.951635  0.000738\n",
       "F1-Score   0.971444  0.000475\n",
       "Precision  0.982941  0.001102\n",
       "Recall     0.960217  0.001894"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not pd.api.types.is_numeric_dtype(df_metrics['Value']):\n",
    "    df_metrics['Value'] = pd.to_numeric(df_metrics['Value'], errors='coerce')\n",
    "\n",
    "pivot_table = df_metrics.pivot_table(index=\"Metric\", values=\"Value\", aggfunc=[\"mean\", \"std\"])\n",
    "df_metrics.pivot_table(index=\"Metric\", values=\"Value\", aggfunc=[\"mean\", \"std\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.boxplot(column='Value', by='Metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MALE/FEMALE:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Equal Opportunity Difference</th>\n",
       "      <td>0.002640</td>\n",
       "      <td>0.001852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalised Odds</th>\n",
       "      <td>-0.011654</td>\n",
       "      <td>0.005981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positive Rate Balance</th>\n",
       "      <td>-0.025949</td>\n",
       "      <td>0.011082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predictive Parity Difference</th>\n",
       "      <td>0.004097</td>\n",
       "      <td>0.002337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Statistical Parity Difference</th>\n",
       "      <td>0.031263</td>\n",
       "      <td>0.069906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   mean       std\n",
       "                                  Value     Value\n",
       "Metric                                           \n",
       "Equal Opportunity Difference   0.002640  0.001852\n",
       "Equalised Odds                -0.011654  0.005981\n",
       "False Positive Rate Balance   -0.025949  0.011082\n",
       "Predictive Parity Difference   0.004097  0.002337\n",
       "Statistical Parity Difference  0.031263  0.069906"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nMALE/FEMALE:\")\n",
    "sex_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "explainer = shap.LinearExplainer(lr, X_train)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False)\n",
    "\n",
    "mv = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "import pandas as pd\n",
    "k, i = True, 1\n",
    "\n",
    "for (train, test) in mv.split(X, y):\n",
    "    # fit model\n",
    "    xgb_clf = xgb_clf.fit(X.iloc[train], y.iloc[train].values.ravel())\n",
    "    \n",
    "    # get predictions in the test set\n",
    "    ypred_prob = xgb_clf.predict_proba(X.iloc[test]).ravel()[1::2] # get probabilities\n",
    "    ypred_class = xgb_clf.predict(X.iloc[test])\n",
    "    # compute performance metrics\n",
    "    metrics = []\n",
    "    for pf in perf_metrics.keys():\n",
    "        if pf in [\"AUC\", \"Brier\"]:\n",
    "            metrics += [[pf, perf_metrics[pf](y.iloc[test].values.ravel(), ypred_prob)]]\n",
    "        else:\n",
    "            metrics += [[pf, perf_metrics[pf](y.iloc[test].values.ravel(), ypred_class)]]\n",
    "\n",
    "    # concatenate results\n",
    "    df_m = pd.DataFrame(metrics, columns=[\"Metric\", \"Value\"])\n",
    "    df_m[\"Fold\"] = i\n",
    "    i += 1\n",
    "    if k:\n",
    "        df_metrics = df_m.copy()\n",
    "        k=0\n",
    "    else:\n",
    "        df_metrics = pd.concat([df_metrics, df_m.copy()], axis=0, ignore_index=True)\n",
    "\n",
    "    sex_metrics = []\n",
    "    for bias in bias_metrics.keys():                                  \n",
    "        sex_metrics.append([bias, bias_metrics[bias](y.iloc[test].values.ravel(), ypred_class,\n",
    "                                        group_e[test], group_f[test])])\n",
    "\n",
    "        # Convert lists to DataFrames before concatenation\n",
    "        sex_df = pd.DataFrame(sex_metrics, columns=[\"Metric\", \"Value\"]).assign(Fold=i)\n",
    "\n",
    "    # Concatenate the new DataFrames with the all metrics DataFrames\n",
    "    sex_metrics_all = pd.concat([sex_metrics_all, sex_df], axis=0)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "sex_summary = sex_metrics_all.pivot_table(index='Metric', values='Value', aggfunc=['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pd.api.types.is_numeric_dtype(df_metrics['Value']):\n",
    "    df_metrics['Value'] = pd.to_numeric(df_metrics['Value'], errors='coerce')\n",
    "\n",
    "pivot_table = df_metrics.pivot_table(index=\"Metric\", values=\"Value\", aggfunc=[\"mean\", \"std\"])\n",
    "df_metrics.pivot_table(index=\"Metric\", values=\"Value\", aggfunc=[\"mean\", \"std\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.boxplot(column='Value', by='Metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MALE/FEMALE:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Equal Opportunity Difference</th>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.001623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalised Odds</th>\n",
       "      <td>-0.011883</td>\n",
       "      <td>0.006916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positive Rate Balance</th>\n",
       "      <td>-0.026274</td>\n",
       "      <td>0.012728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predictive Parity Difference</th>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.002642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Statistical Parity Difference</th>\n",
       "      <td>0.031263</td>\n",
       "      <td>0.065908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   mean       std\n",
       "                                  Value     Value\n",
       "Metric                                           \n",
       "Equal Opportunity Difference   0.002508  0.001623\n",
       "Equalised Odds                -0.011883  0.006916\n",
       "False Positive Rate Balance   -0.026274  0.012728\n",
       "Predictive Parity Difference   0.004120  0.002642\n",
       "Statistical Parity Difference  0.031263  0.065908"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nMALE/FEMALE:\")\n",
    "sex_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "explainer = shap.LinearExplainer(xgb_clf, X_train)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "mv = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "import pandas as pd\n",
    "k, i = True, 1\n",
    "\n",
    "for (train, test) in mv.split(X, y):\n",
    "    # fit model\n",
    "    random_forest = random_forest.fit(X.iloc[train], y.iloc[train].values.ravel())\n",
    "    \n",
    "    # get predictions in the test set\n",
    "    ypred_prob = random_forest.predict_proba(X.iloc[test]).ravel()[1::2] # get probabilities\n",
    "    ypred_class = random_forest.predict(X.iloc[test])\n",
    "    # compute performance metrics\n",
    "    metrics = []\n",
    "    for pf in perf_metrics.keys():\n",
    "        if pf == \"AUC\":\n",
    "            metrics += [[pf, perf_metrics[pf](y.iloc[test].values.ravel(), ypred_prob)]]\n",
    "        else:\n",
    "            metrics += [[pf, perf_metrics[pf](y.iloc[test].values.ravel(), ypred_class)]]\n",
    "\n",
    "    # concatenate results\n",
    "    df_m = pd.DataFrame(metrics, columns=[\"Metric\", \"Value\"])\n",
    "    df_m[\"Fold\"] = i\n",
    "    i += 1\n",
    "    if k:\n",
    "        df_metrics = df_m.copy()\n",
    "        k=0\n",
    "    else:\n",
    "        df_metrics = pd.concat([df_metrics, df_m.copy()], axis=0, ignore_index=True)\n",
    "\n",
    "    # compute performance metrics\n",
    "    sex_metrics = []\n",
    "    for bias in bias_metrics.keys():                                \n",
    "        sex_metrics.append([bias, bias_metrics[bias](y.iloc[test].values.ravel(), ypred_class,\n",
    "                                        group_e[test], group_f[test])])\n",
    "        # Convert lists to DataFrames before concatenation\n",
    "        sex_df = pd.DataFrame(sex_metrics, columns=[\"Metric\", \"Value\"]).assign(Fold=i)\n",
    " \n",
    "\n",
    "    # Concatenate the new DataFrames with the all metrics DataFrames\n",
    "    sex_metrics_all = pd.concat([sex_metrics_all, sex_df], axis=0)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "sex_summary = sex_metrics_all.pivot_table(index='Metric', values='Value', aggfunc=['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not pd.api.types.is_numeric_dtype(df_metrics['Value']):\n",
    "    df_metrics['Value'] = pd.to_numeric(df_metrics['Value'], errors='coerce')\n",
    "\n",
    "pivot_table = df_metrics.pivot_table(index=\"Metric\", values=\"Value\", aggfunc=[\"mean\", \"std\"])\n",
    "df_metrics.pivot_table(index=\"Metric\", values=\"Value\", aggfunc=[\"mean\", \"std\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.boxplot(column='Value', by='Metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMALE/FEMALE:\")\n",
    "sex_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "explainer = shap.LinearExplainer(random_forest, X_train)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "k, i = True, 1\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "svm = svm.SVC(random_state=10, probability=True)\n",
    "\n",
    "for (train, test) in mv.split(X, y):\n",
    "    svm = svm.fit(X.iloc[train], y.iloc[train].values.ravel())\n",
    "    \n",
    "    ypred_prob = svm.predict_proba(X.iloc[test]).ravel()[1::2] # get probabilities\n",
    "    ypred_class = svm.predict(X.iloc[test])\n",
    "    # compute performance metrics\n",
    "    metrics = []\n",
    "    for pf in perf_metrics.keys():\n",
    "        if pf in [\"AUC\", \"Brier\"]:\n",
    "            metrics += [[pf, perf_metrics[pf](y.iloc[test].values.ravel(), ypred_prob)]]\n",
    "        else:\n",
    "            metrics += [[pf, perf_metrics[pf](y.iloc[test].values.ravel(), ypred_class)]]\n",
    "\n",
    "    # concatenate results\n",
    "    df_m = pd.DataFrame(metrics, columns=[\"Metric\", \"Value\"])\n",
    "    df_m[\"Fold\"] = i\n",
    "    i += 1\n",
    "    if k:\n",
    "        df_metrics = df_m.copy()\n",
    "        k=0\n",
    "    else:\n",
    "        df_metrics = pd.concat([df_metrics, df_m.copy()], axis=0, ignore_index=True)\n",
    "\n",
    "    sex_metrics = []\n",
    "    for bias in bias_metrics.keys():                                   \n",
    "        sex_metrics.append([bias, bias_metrics[bias](y.iloc[test].values.ravel(), ypred_class,\n",
    "                                        group_e[test], group_f[test])])\n",
    "        # Convert lists to DataFrames before concatenation\n",
    "        sex_df = pd.DataFrame(sex_metrics, columns=[\"Metric\", \"Value\"]).assign(Fold=i)\n",
    "\n",
    "\n",
    "    # Concatenate the new DataFrames with the all metrics DataFrames\n",
    "    sex_metrics_all = pd.concat([sex_metrics_all, sex_df], axis=0)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "sex_summary = sex_metrics_all.pivot_table(index='Metric', values='Value', aggfunc=['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pd.api.types.is_numeric_dtype(df_metrics['Value']):\n",
    "    df_metrics['Value'] = pd.to_numeric(df_metrics['Value'], errors='coerce')\n",
    "\n",
    "pivot_table = df_metrics.pivot_table(index=\"Metric\", values=\"Value\", aggfunc=[\"mean\", \"std\"])\n",
    "df_metrics.pivot_table(index=\"Metric\", values=\"Value\", aggfunc=[\"mean\", \"std\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.boxplot(column='Value', by='Metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMALE/FEMALE:\")\n",
    "sex_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "explainer = shap.LinearExplainer(svm, X_train)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "parameters = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "lgb_model = lgb.train(parameters,\n",
    "                  train_data,\n",
    "                  valid_sets=[valid_data],\n",
    "                  num_boost_round=5000)\n",
    "\n",
    "mv = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "import pandas as pd\n",
    "k, i = True, 1\n",
    "\n",
    "for (train, test) in mv.split(X, y):\n",
    "    # fit model\n",
    "    lgb_model = lgb_model.fit(X.iloc[train], y.iloc[train].values.ravel())\n",
    "    \n",
    "    # get predictions in the test set\n",
    "    ypred_prob = lgb_model.predict_proba(X.iloc[test]).ravel()[1::2] # get probabilities\n",
    "    ypred_class = lgb_model.predict(X.iloc[test])\n",
    "    # compute performance metrics\n",
    "    metrics = []\n",
    "    for pf in perf_metrics.keys():\n",
    "        if pf in [\"AUC\", \"Brier\"]:\n",
    "            metrics += [[pf, perf_metrics[pf](y.iloc[test].values.ravel(), ypred_prob)]]\n",
    "        else:\n",
    "            metrics += [[pf, perf_metrics[pf](y.iloc[test].values.ravel(), ypred_class)]]\n",
    "\n",
    "    # concatenate results\n",
    "    df_m = pd.DataFrame(metrics, columns=[\"Metric\", \"Value\"])\n",
    "    df_m[\"Fold\"] = i\n",
    "    i += 1\n",
    "    if k:\n",
    "        df_metrics = df_m.copy()\n",
    "        k=0\n",
    "    else:\n",
    "        df_metrics = pd.concat([df_metrics, df_m.copy()], axis=0, ignore_index=True)\n",
    "\n",
    "    # compute performance metrics\n",
    "    sex_metrics = []\n",
    "    for bias in bias_metrics.keys():                             \n",
    "        sex_metrics.append([bias, bias_metrics[bias](y.iloc[test].values.ravel(), ypred_class,\n",
    "                                        group_e[test], group_f[test])])\n",
    "        # Convert lists to DataFrames before concatenation\n",
    "        sex_df = pd.DataFrame(sex_metrics, columns=[\"Metric\", \"Value\"]).assign(Fold=i)\n",
    "\n",
    "\n",
    "    # Concatenate the new DataFrames with the all metrics DataFrames\n",
    "    sex_metrics_all = pd.concat([sex_metrics_all, sex_df], axis=0)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "sex_summary = sex_metrics_all.pivot_table(index='Metric', values='Value', aggfunc=['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pd.api.types.is_numeric_dtype(df_metrics['Value']):\n",
    "    df_metrics['Value'] = pd.to_numeric(df_metrics['Value'], errors='coerce')\n",
    "\n",
    "pivot_table = df_metrics.pivot_table(index=\"Metric\", values=\"Value\", aggfunc=[\"mean\", \"std\"])\n",
    "df_metrics.pivot_table(index=\"Metric\", values=\"Value\", aggfunc=[\"mean\", \"std\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.boxplot(column='Value', by='Metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMALE/FEMALE:\")\n",
    "sex_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "explainer = shap.LinearExplainer(lgb_model, X_train)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab01467e1ba0f853da7a9f5b7c73c9bf62ddfc76943c22f8ed9be678f3a21eaa"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit ('py3cv4': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
